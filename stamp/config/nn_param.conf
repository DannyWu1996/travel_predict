# coding=utf-8
# [model]
# str: "abc", not 'abc', 
# don't use \"
# extend: a:b:c means a extend from bï¼Œc.

[seq2seqv2nn]
{
    "dataset" : "rsc15",
    "nepoch" : 50,
    "batch_size" : 300,
    "init_lr" : 0.001,
    "stddev" : 0.05,
    "emb_stddev" : 0.002,
    "edim" : 300,
    "max_grad_norm" : 150,  # 150 if set None, while not clip the grads.
    "pad_idx" : 0,
    "emb_up" : false,  # should update the pre-train embedding.
    "update_lr" : false,
    "active" : "sigmoid",
    "model_save_path" : "./ckpt/",
    "base_threshold_acc" : 0.5
}

[base_stamp : seq2seqv2nn]
{
    "nepoch" : 20,
    "is_print" : true,
    "batch_size" : 256,
    "init_lr" : 0.0003,
    "max_grad_norm" :110,
    "active" : "sigmoid",
    "cell" : "gru",
    "hidden_size" : 100,
    "edim" : 100,
    "emb_up": true,
    "cut_off": 5
}

